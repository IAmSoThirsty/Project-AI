#!/usr/bin/env python3
"""
Historical Documentation Organizer

Identifies and moves obsolete/outdated documentation to a historical archive folder.
Also removes references to these documents from active files.

Features:
- Pattern-based identification of completion/status/implementation docs
- Safe file moving with preservation of git history
- Reference removal from active documentation
- Comprehensive logging and reporting
"""

import argparse
import json
import logging
import os
import re
import shutil
import sys
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Set, Tuple

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(sys.stdout),
        logging.FileHandler('historical_docs_organizer.log')
    ]
)
logger = logging.getLogger(__name__)

# Patterns for identifying obsolete documents
OBSOLETE_PATTERNS = [
    r'.*_COMPLETE\.md$',
    r'.*_SUMMARY\.md$',
    r'.*_IMPLEMENTATION\.md$',
    r'.*_STATUS\.md$',
    r'.*_ANALYSIS\.md$',
    r'.*_FIXES\.md$',
    r'.*PATCH.*\.md$',
    r'.*_QUICKSTART\.md$',
    r'.*_SUCCESS\.md$',
    r'.*_CHECKLIST\.md$',
    r'.*_GUIDE\.md$',
    r'.*_SOLUTIONS\.md$',
    r'.*_RECOMMENDATIONS\.md$',
    r'.*_ISSUES\.md$',
    r'.*_AUDIT\.md$',
]

# Documents to ALWAYS keep (not move to historical)
KEEP_DOCUMENTS = [
    'README.md',
    'CHANGELOG.md',
    'CONTRIBUTING.md',
    'CODE_OF_CONDUCT.md',
    'SECURITY.md',
    'LICENSE.md',
    'LICENSE.txt',
    'CONSTITUTION.md',
    'COMPLETE_TEST_SUITE_SUMMARY.md',  # Will be updated by test report system
    'DEVELOPER_QUICK_REFERENCE.md',
    'PROGRAM_SUMMARY.md',
    'TECHNICAL_WHITE_PAPER.md',
    'test_documentation_update_summary.md',  # Generated by test report updater
    'security_test_category_pass_fail_report.md',  # Generated by test report updater
    'historical_docs_organization_report.md',  # Generated by this script
]

# Keywords that suggest a document is historical/completed
HISTORICAL_KEYWORDS = [
    'complete',
    'completed',
    'implementation complete',
    'summary',
    'patch',
    'fixed',
    'success',
    'quickstart',
    'analysis',
    'audit',
]

class HistoricalDocsOrganizer:
    """Organize historical documentation."""
    
    def __init__(self, dry_run: bool = False, interactive: bool = False, check_age: bool = True, age_threshold_days: int = 7):
        self.dry_run = dry_run
        self.interactive = interactive
        self.check_age = check_age
        self.age_threshold_days = age_threshold_days
        self.logger = logging.getLogger(f"{__name__}.HistoricalDocsOrganizer")
        self.historical_dir = Path("docs/historical")
        self.moved_files: List[Tuple[str, str]] = []
        self.kept_files: List[str] = []
        self.references_updated: Dict[str, int] = {}
        self.age_checked_files: List[Tuple[str, int]] = []  # (filename, age_in_days)
    
    def identify_obsolete_documents(self) -> List[Path]:
        """Identify documents that should be moved to historical."""
        self.logger.info("Identifying obsolete documents...")
        
        obsolete_docs = []
        root_path = Path(".")
        current_time = datetime.now()
        age_threshold = timedelta(days=self.age_threshold_days)
        
        # Find all markdown files in root
        for md_file in root_path.glob("*.md"):
            filename = md_file.name
            
            # Skip if in keep list
            if filename in KEEP_DOCUMENTS:
                self.kept_files.append(filename)
                continue
            
            # Check file age
            file_age_days = 0
            is_old = False
            
            if self.check_age:
                try:
                    file_stat = md_file.stat()
                    file_mtime = datetime.fromtimestamp(file_stat.st_mtime)
                    file_age = current_time - file_mtime
                    file_age_days = file_age.days
                    
                    if file_age > age_threshold:
                        is_old = True
                        self.age_checked_files.append((filename, file_age_days))
                
                except Exception as e:
                    self.logger.warning(f"Could not check age of {md_file}: {e}")
            
            # Check against patterns
            is_obsolete = False
            matched_pattern = None
            
            for pattern in OBSOLETE_PATTERNS:
                if re.match(pattern, filename, re.IGNORECASE):
                    is_obsolete = True
                    matched_pattern = pattern
                    break
            
            # Check content for historical keywords if not already matched
            if not is_obsolete:
                try:
                    with open(md_file, 'r', encoding='utf-8', errors='ignore') as f:
                        content = f.read(1000).lower()  # Check first 1000 chars
                    
                    # Count historical keywords
                    keyword_count = sum(1 for keyword in HISTORICAL_KEYWORDS if keyword in content)
                    
                    if keyword_count >= 2:
                        is_obsolete = True
                        matched_pattern = f"content keywords (count: {keyword_count})"
                
                except Exception as e:
                    self.logger.warning(f"Could not read {md_file}: {e}")
            
            # Age-based assessment: if file is old and not explicitly useful, consider it obsolete
            if not is_obsolete and is_old:
                # Check if file appears to be actively maintained
                is_useful = self._assess_usefulness(md_file, filename)
                
                if not is_useful:
                    is_obsolete = True
                    matched_pattern = f"age-based ({file_age_days} days old, not actively maintained)"
                    self.logger.info(f"  Age assessment: {filename} is {file_age_days} days old and not actively maintained")
            
            if is_obsolete:
                self.logger.info(f"  Obsolete: {filename} (matched: {matched_pattern})")
                obsolete_docs.append(md_file)
            else:
                if is_old:
                    self.logger.info(f"  Keeping: {filename} ({file_age_days} days old but actively maintained/useful)")
                self.kept_files.append(filename)
        
        self.logger.info(f"Found {len(obsolete_docs)} obsolete documents, keeping {len(self.kept_files)}")
        
        return obsolete_docs
    
    def _assess_usefulness(self, filepath: Path, filename: str) -> bool:
        """Assess if a file is useful and should be kept despite age.
        
        Returns True if file should be kept, False if it should be moved to historical.
        """
        try:
            with open(filepath, 'r', encoding='utf-8', errors='ignore') as f:
                content = f.read()
            
            content_lower = content.lower()
            
            # Files that are clearly useful/active
            useful_indicators = [
                'table of contents',
                'getting started',
                'installation',
                'usage',
                'api reference',
                'architecture',
                'design',
                'specification',
                'requirements',
                'roadmap',
                'features',
                'documentation',
                'tutorial',
                'examples',
                'best practices',
                'troubleshooting',
            ]
            
            # Files that are clearly historical/completed
            obsolete_indicators = [
                'completed on',
                'implementation complete',
                'task completed',
                'successfully implemented',
                'final status:',
                'archived',
                'deprecated',
                'superseded by',
                'no longer maintained',
                'obsolete',
                'legacy',
            ]
            
            # Count indicators
            useful_count = sum(1 for indicator in useful_indicators if indicator in content_lower)
            obsolete_count = sum(1 for indicator in obsolete_indicators if indicator in content_lower)
            
            # Decision logic
            if obsolete_count >= 2:
                return False  # Clearly obsolete
            
            if useful_count >= 3:
                return True  # Clearly useful
            
            # Check if file has recent updates marker
            if 'updated:' in content_lower or 'last modified:' in content_lower:
                # Try to extract date
                date_patterns = [
                    r'updated:\s*(\d{4}-\d{2}-\d{2})',
                    r'last modified:\s*(\d{4}-\d{2}-\d{2})',
                    r'date:\s*(\d{4}-\d{2}-\d{2})',
                ]
                
                for pattern in date_patterns:
                    match = re.search(pattern, content_lower)
                    if match:
                        try:
                            update_date = datetime.strptime(match.group(1), '%Y-%m-%d')
                            days_since_update = (datetime.now() - update_date).days
                            
                            if days_since_update <= self.age_threshold_days:
                                return True  # Recently updated
                        except:
                            pass
            
            # Check file size - very small files might be stubs/placeholders
            file_size = filepath.stat().st_size
            if file_size < 500:  # Less than 500 bytes
                return False  # Likely a stub or minimal file
            
            # Default: if uncertain but has some useful content, keep it
            if useful_count > 0:
                return True
            
            # Default: if no clear indicators, consider it not useful
            return False
        
        except Exception as e:
            self.logger.warning(f"Error assessing usefulness of {filepath}: {e}")
            return True  # If we can't assess, keep it to be safe
    
    def move_to_historical(self, files: List[Path]) -> None:
        """Move files to historical directory."""
        if not files:
            self.logger.info("No files to move")
            return
        
        # Create historical directory
        if not self.dry_run:
            self.historical_dir.mkdir(parents=True, exist_ok=True)
            self.logger.info(f"Created historical directory: {self.historical_dir}")
        
        # Move files
        for filepath in files:
            dest_path = self.historical_dir / filepath.name
            
            if self.interactive:
                response = input(f"Move {filepath} to {dest_path}? [y/N]: ")
                if response.lower() != 'y':
                    self.logger.info(f"  Skipped: {filepath}")
                    continue
            
            if self.dry_run:
                self.logger.info(f"  DRY RUN: Would move {filepath} -> {dest_path}")
                self.moved_files.append((str(filepath), str(dest_path)))
            else:
                try:
                    shutil.move(str(filepath), str(dest_path))
                    self.logger.info(f"  Moved: {filepath} -> {dest_path}")
                    self.moved_files.append((str(filepath), str(dest_path)))
                except Exception as e:
                    self.logger.error(f"  Error moving {filepath}: {e}")
    
    def find_references(self, moved_files: List[Tuple[str, str]]) -> Dict[str, List[Tuple[str, int]]]:
        """Find references to moved files in active documentation."""
        self.logger.info("Finding references to moved files...")
        
        # Extract just the filenames that were moved
        moved_filenames = [Path(src).name for src, _ in moved_files]
        
        references = {}
        
        # Search in remaining markdown files
        for md_file in Path(".").glob("*.md"):
            if md_file.name in moved_filenames:
                continue  # Skip the moved files themselves
            
            try:
                with open(md_file, 'r', encoding='utf-8', errors='ignore') as f:
                    lines = f.readlines()
                
                file_references = []
                
                for line_num, line in enumerate(lines, 1):
                    for moved_filename in moved_filenames:
                        # Check for various reference formats
                        if (moved_filename in line or 
                            moved_filename.replace('.md', '') in line):
                            file_references.append((line.strip(), line_num))
                
                if file_references:
                    references[str(md_file)] = file_references
                    self.logger.info(f"  Found {len(file_references)} references in {md_file}")
            
            except Exception as e:
                self.logger.warning(f"Could not read {md_file}: {e}")
        
        # Also search in common documentation directories
        for doc_dir in ['docs', '.github', 'api']:
            doc_path = Path(doc_dir)
            if doc_path.exists():
                for md_file in doc_path.rglob("*.md"):
                    try:
                        with open(md_file, 'r', encoding='utf-8', errors='ignore') as f:
                            lines = f.readlines()
                        
                        file_references = []
                        
                        for line_num, line in enumerate(lines, 1):
                            for moved_filename in moved_filenames:
                                if (moved_filename in line or 
                                    moved_filename.replace('.md', '') in line):
                                    file_references.append((line.strip(), line_num))
                        
                        if file_references:
                            references[str(md_file)] = file_references
                            self.logger.info(f"  Found {len(file_references)} references in {md_file}")
                    
                    except Exception as e:
                        self.logger.warning(f"Could not read {md_file}: {e}")
        
        return references
    
    def update_references(self, references: Dict[str, List[Tuple[str, int]]]) -> None:
        """Update references to point to historical directory."""
        self.logger.info("Updating references...")
        
        for filepath, ref_list in references.items():
            if self.dry_run:
                self.logger.info(f"  DRY RUN: Would update {len(ref_list)} references in {filepath}")
                self.references_updated[filepath] = len(ref_list)
                continue
            
            try:
                with open(filepath, 'r', encoding='utf-8') as f:
                    content = f.read()
                
                original_content = content
                updates_made = 0
                
                # Extract moved filenames for replacement
                moved_filenames = [Path(src).name for src, _ in self.moved_files]
                
                for moved_filename in moved_filenames:
                    # Update markdown links
                    # [text](FILENAME.md) -> [text](docs/historical/FILENAME.md)
                    pattern = r'\[([^\]]+)\]\(' + re.escape(moved_filename) + r'\)'
                    replacement = r'[\1](docs/historical/' + moved_filename + ')'
                    new_content = re.sub(pattern, replacement, content)
                    
                    if new_content != content:
                        updates_made += 1
                        content = new_content
                    
                    # Update direct references
                    # FILENAME.md -> docs/historical/FILENAME.md (if not already a link)
                    # But avoid double-updating
                    if 'docs/historical/' + moved_filename not in content:
                        pattern = r'(?<!\[)(?<!\()' + re.escape(moved_filename) + r'(?!\))'
                        replacement = 'docs/historical/' + moved_filename
                        new_content = re.sub(pattern, replacement, content)
                        
                        if new_content != content:
                            updates_made += 1
                            content = new_content
                
                if content != original_content:
                    with open(filepath, 'w', encoding='utf-8') as f:
                        f.write(content)
                    
                    self.logger.info(f"  Updated {updates_made} references in {filepath}")
                    self.references_updated[filepath] = updates_made
            
            except Exception as e:
                self.logger.error(f"Error updating {filepath}: {e}")
    
    def create_historical_index(self) -> None:
        """Create an index file in the historical directory."""
        if self.dry_run:
            self.logger.info("DRY RUN: Would create historical index")
            return
        
        index_path = self.historical_dir / "README.md"
        
        self.logger.info(f"Creating historical index: {index_path}")
        
        content = f"""# Historical Documentation

This directory contains historical, obsolete, or completed documentation that is no longer actively maintained but preserved for reference.

**Archive Date:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

## Archived Documents

"""
        
        # List moved files
        for src, dest in sorted(self.moved_files):
            filename = Path(dest).name
            content += f"- [{filename}]({filename})\n"
        
        content += f"""

## Notes

- These documents were automatically archived by the Historical Documentation Organizer
- They represent completed implementations, summaries, or status reports from prior development phases
- For current documentation, see the main repository README and active documentation files
- Total files archived: {len(self.moved_files)}

## Restoration

If you need to restore any of these documents to active status:

1. Move the file back to the repository root: `git mv docs/historical/FILENAME.md ./`
2. Update any references in active documentation
3. Ensure the content is current and relevant

---

*Generated by Historical Documentation Organizer v1.0.0*
"""
        
        with open(index_path, 'w', encoding='utf-8') as f:
            f.write(content)
        
        self.logger.info("Historical index created")
    
    def generate_report(self) -> str:
        """Generate summary report."""
        report = f"""# Historical Documentation Organization Report

**Generated:** {datetime.now().isoformat()}
**Mode:** {'DRY RUN' if self.dry_run else 'PRODUCTION'}
**Age Checking:** {'Enabled' if self.check_age else 'Disabled'}
**Age Threshold:** {self.age_threshold_days} days

## Summary

- **Files moved to historical:** {len(self.moved_files)}
- **Files kept active:** {len(self.kept_files)}
- **Files checked for age:** {len(self.age_checked_files)}
- **References updated:** {sum(self.references_updated.values())} across {len(self.references_updated)} files

## Age Assessment

Files older than {self.age_threshold_days} days that were checked:

"""
        
        if self.age_checked_files:
            for filename, age_days in sorted(self.age_checked_files, key=lambda x: x[1], reverse=True):
                status = "ðŸ“¦ Archived" if any(filename in src for src, _ in self.moved_files) else "âœ… Kept (useful)"
                report += f"- `{filename}`: {age_days} days old - {status}\n"
        else:
            report += "*No files older than threshold found*\n"
        
        report += "\n## Moved Files\n\n"
        
        for src, dest in sorted(self.moved_files):
            report += f"- `{src}` â†’ `{dest}`\n"
        
        report += "\n## Reference Updates\n\n"
        
        if self.references_updated:
            for filepath, count in sorted(self.references_updated.items()):
                report += f"- `{filepath}`: {count} references updated\n"
        else:
            report += "*No references needed updating*\n"
        
        report += f"""

## Active Documents

The following documents remain active in the root directory:

"""
        
        for filename in sorted(self.kept_files):
            report += f"- {filename}\n"
        
        report += """

---

*Generated by Historical Documentation Organizer v1.0.0*
"""
        
        return report


def main():
    """Main entry point."""
    parser = argparse.ArgumentParser(
        description='Historical Documentation Organizer',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # Dry run to see what would be moved
  python organize_historical_docs.py --dry-run

  # Interactive mode (ask before moving each file)
  python organize_historical_docs.py --interactive

  # Check files older than 7 days (default)
  python organize_historical_docs.py --check-age

  # Check files older than 14 days
  python organize_historical_docs.py --check-age --age-threshold 14

  # Disable age checking
  python organize_historical_docs.py --no-check-age

  # Verbose output
  python organize_historical_docs.py --verbose
        """
    )
    
    parser.add_argument('--dry-run', '-d', action='store_true',
                       help='Run without making changes')
    parser.add_argument('--interactive', '-i', action='store_true',
                       help='Ask before moving each file')
    parser.add_argument('--check-age', action='store_true', default=True,
                       help='Check file age and assess usefulness (default: enabled)')
    parser.add_argument('--no-check-age', action='store_false', dest='check_age',
                       help='Disable age-based checking')
    parser.add_argument('--age-threshold', type=int, default=7,
                       help='Age threshold in days (default: 7)')
    parser.add_argument('--verbose', '-v', action='store_true',
                       help='Enable verbose logging')
    
    args = parser.parse_args()
    
    # Set logging level
    if args.verbose:
        logging.getLogger().setLevel(logging.DEBUG)
    
    # Create organizer
    organizer = HistoricalDocsOrganizer(
        dry_run=args.dry_run,
        interactive=args.interactive,
        check_age=args.check_age,
        age_threshold_days=args.age_threshold
    )
    
    # Identify obsolete documents
    logger.info("=" * 80)
    logger.info("IDENTIFYING OBSOLETE DOCUMENTS")
    logger.info("=" * 80)
    
    obsolete_docs = organizer.identify_obsolete_documents()
    
    if not obsolete_docs:
        logger.info("No obsolete documents found")
        return 0
    
    # Move to historical
    logger.info("=" * 80)
    logger.info("MOVING TO HISTORICAL DIRECTORY")
    logger.info("=" * 80)
    
    organizer.move_to_historical(obsolete_docs)
    
    # Find and update references
    if organizer.moved_files:
        logger.info("=" * 80)
        logger.info("FINDING REFERENCES")
        logger.info("=" * 80)
        
        references = organizer.find_references(organizer.moved_files)
        
        if references:
            logger.info("=" * 80)
            logger.info("UPDATING REFERENCES")
            logger.info("=" * 80)
            
            organizer.update_references(references)
        
        # Create historical index
        logger.info("=" * 80)
        logger.info("CREATING HISTORICAL INDEX")
        logger.info("=" * 80)
        
        organizer.create_historical_index()
    
    # Generate report
    logger.info("=" * 80)
    logger.info("GENERATING REPORT")
    logger.info("=" * 80)
    
    report = organizer.generate_report()
    
    report_path = "historical_docs_organization_report.md"
    with open(report_path, 'w', encoding='utf-8') as f:
        f.write(report)
    
    logger.info(f"Report saved to {report_path}")
    
    # Summary
    logger.info("=" * 80)
    logger.info("ORGANIZATION COMPLETE")
    logger.info("=" * 80)
    logger.info(f"Files moved: {len(organizer.moved_files)}")
    logger.info(f"References updated: {sum(organizer.references_updated.values())}")
    
    return 0


if __name__ == '__main__':
    sys.exit(main())
