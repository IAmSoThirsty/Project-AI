# ==============================================================================
# PostgreSQL Configuration - Project-AI Core Stack
# ==============================================================================
#
# Production-optimized PostgreSQL configuration for AI workloads with pgvector.
# This file is optional - defaults are generally sufficient for single-node.
# Mount this file in docker-compose.yml for custom configuration.
#
# ==============================================================================

# ==============================================================================
# CONNECTION AND AUTHENTICATION
# ==============================================================================
listen_addresses = '*'
max_connections = 200
superuser_reserved_connections = 3

# ==============================================================================
# RESOURCE USAGE (MEMORY)
# ==============================================================================

# Memory configuration (adjust based on available RAM)
# Rule of thumb: shared_buffers = 25% of RAM, effective_cache_size = 50-75% of RAM
shared_buffers = 256MB              # Start small, monitor and adjust
effective_cache_size = 1GB           # OS cache + shared_buffers estimate
work_mem = 16MB                      # Per operation memory
maintenance_work_mem = 128MB         # For VACUUM, CREATE INDEX, etc.

# Huge pages (uncomment for large memory systems)
# huge_pages = try

# ==============================================================================
# RESOURCE USAGE (DISK)
# ==============================================================================

# Temporary files
temp_buffers = 16MB
temp_file_limit = -1

# ==============================================================================
# WRITE-AHEAD LOG (WAL)
# ==============================================================================

# WAL configuration for durability
wal_level = replica                  # or 'logical' for logical replication
fsync = on
synchronous_commit = on
wal_sync_method = fdatasync
full_page_writes = on
wal_compression = on
wal_buffers = 16MB

# Checkpoints
checkpoint_timeout = 10min
checkpoint_completion_target = 0.9
max_wal_size = 2GB
min_wal_size = 80MB

# Archive (for point-in-time recovery)
archive_mode = off
# archive_command = 'test ! -f /path/to/archive/%f && cp %p /path/to/archive/%f'

# ==============================================================================
# REPLICATION (for future HA setup)
# ==============================================================================

max_wal_senders = 5
max_replication_slots = 5
hot_standby = on
hot_standby_feedback = off

# ==============================================================================
# QUERY TUNING
# ==============================================================================

# Planner cost constants (adjust for SSD)
random_page_cost = 1.1               # Lower for SSD (default: 4.0)
effective_io_concurrency = 200       # Higher for SSD (default: 1)
seq_page_cost = 1.0

# Parallelism
max_worker_processes = 8
max_parallel_workers_per_gather = 4
max_parallel_workers = 8
max_parallel_maintenance_workers = 4

# Join and sort
enable_hashjoin = on
enable_mergejoin = on
enable_nestloop = on
enable_seqscan = on
enable_indexscan = on
enable_indexonlyscan = on

# ==============================================================================
# STATISTICS AND MONITORING
# ==============================================================================

# Query planning
default_statistics_target = 100

# Logging for slow queries
log_min_duration_statement = 1000    # Log queries > 1 second
log_line_prefix = '%t [%p]: [%l-1] user=%u,db=%d,app=%a,client=%h '
log_checkpoints = on
log_connections = on
log_disconnections = on
log_duration = off
log_lock_waits = on

# pg_stat_statements (query performance tracking)
shared_preload_libraries = 'pg_stat_statements'
pg_stat_statements.max = 10000
pg_stat_statements.track = all

# ==============================================================================
# AUTOVACUUM (critical for performance)
# ==============================================================================

autovacuum = on
autovacuum_max_workers = 3
autovacuum_naptime = 1min
autovacuum_vacuum_threshold = 50
autovacuum_analyze_threshold = 50
autovacuum_vacuum_scale_factor = 0.1
autovacuum_analyze_scale_factor = 0.05
autovacuum_vacuum_cost_delay = 10ms
autovacuum_vacuum_cost_limit = 200

# ==============================================================================
# CLIENT CONNECTION DEFAULTS
# ==============================================================================

# Locale
datestyle = 'iso, mdy'
timezone = 'UTC'
lc_messages = 'en_US.utf8'
lc_monetary = 'en_US.utf8'
lc_numeric = 'en_US.utf8'
lc_time = 'en_US.utf8'
default_text_search_config = 'pg_catalog.english'

# Statement behavior
statement_timeout = 0                # No timeout (set per-query if needed)
lock_timeout = 0
idle_in_transaction_session_timeout = 0

# ==============================================================================
# EXTENSIONS (pgvector optimization)
# ==============================================================================

# pgvector-specific tuning
# Increase maintenance_work_mem for faster index builds
# Use HNSW index for better performance than IVFFlat at scale

# Example index creation:
# CREATE INDEX ON vector_memory USING hnsw (embedding vector_cosine_ops);

# ==============================================================================
# LOCK MANAGEMENT
# ==============================================================================

deadlock_timeout = 1s
max_locks_per_transaction = 64

# ==============================================================================
# ERROR HANDLING
# ==============================================================================

restart_after_crash = on
exit_on_error = off

# ==============================================================================
# CUSTOM SETTINGS FOR PROJECT-AI
# ==============================================================================

# JIT compilation (may improve complex query performance)
jit = on
jit_above_cost = 100000
jit_inline_above_cost = 500000
jit_optimize_above_cost = 500000

# ==============================================================================
# NOTES AND RECOMMENDATIONS
# ==============================================================================
#
# MEMORY TUNING:
# 1. Monitor with pg_stat_statements and adjust work_mem
# 2. shared_buffers should be 25% of RAM (max 8GB typically)
# 3. effective_cache_size should be 50-75% of total RAM
# 4. Increase maintenance_work_mem for index builds
#
# PGVECTOR OPTIMIZATION:
# 1. Use HNSW indexes for better performance: USING hnsw (embedding vector_cosine_ops)
# 2. Increase maintenance_work_mem before building vector indexes
# 3. Consider partitioning large vector tables by date/category
# 4. Monitor with: SELECT * FROM pg_stat_user_indexes WHERE schemaname = 'public';
#
# PERFORMANCE MONITORING:
# 1. Enable pg_stat_statements (already configured)
# 2. Query: SELECT * FROM pg_stat_statements ORDER BY total_exec_time DESC LIMIT 10;
# 3. Monitor cache hit rate: SELECT * FROM pg_stat_database WHERE datname = 'project_ai';
# 4. Watch autovacuum: SELECT * FROM pg_stat_user_tables;
#
# CONNECTION POOLING:
# Consider using PgBouncer for connection pooling:
# - docker run -d --name pgbouncer -p 6432:6432 pgbouncer/pgbouncer
#
# BACKUP STRATEGY:
# 1. Daily pg_dump: pg_dump -U project_ai project_ai > backup.sql
# 2. Enable WAL archiving for PITR (point-in-time recovery)
# 3. Use pg_basebackup for physical backups
#
# HIGH AVAILABILITY:
# For production HA, consider:
# 1. Streaming replication with standby servers
# 2. Patroni for automatic failover
# 3. pgBouncer for connection pooling
# 4. Citus for sharding (if scaling beyond single node)
#
# ==============================================================================
